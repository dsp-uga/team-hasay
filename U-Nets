import numpy as np
import os
from keras.models import * 
from keras.layers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from scipy.ndimage.filters import *
from keras.optimizers import Adam
import cv2
import matplotlib.pyplot as plt

def unet(input_height, input_width, n_classes):
    input = Input(shape=(input_height, input_width, n_classes))
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    dropout4 = Dropout(0.5)(conv4)
    conv4 = MaxPooling2D(pool_size=(2, 2))(dropout4)

    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    dropout5 = Dropout(0.5)(conv5)

    upsampling = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge  = concatenate([dropout4,upsampling], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

    upsampling = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge = concatenate([conv3,upsampling], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    
    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)
    conv10 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv10 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv11 = Conv2D(3, 1, activation = 'sigmoid')(conv9)

    model = Model(input = inputs, output = conv10)
    model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model
    
    
def fourier_transform(img):
    f = np.fft.fft2(img)
    fshift = np.fft.fftshift(f)
    img_magnitude_spectrum = 20 * np.log(np.abs(fshift))
    return img_magnitude_spectrum

def laplacian(img):
    laplacian = cv2.Laplacian(img, cv2.CV_64F)
    #laplacian = laplacian[:, :, :1]
    return laplacian

def canny(img):
    canny = cv2.Canny(img, 100, 200)
    return np.expand_dims(canny, axis=2)

def normalize(img):
    mean = np.mean(img)
    std = np.std(img)
    row = img.shape[0]
    col = img.shape[1]
    for i in range(row):
        for j in range(col):
            img[i][j][0] = (img[i][j][0] - mean) / std
    return img[:, :, :1]
    
def load_training_data():
    train_names = open('C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//team-hasay-master//train.txt').read().split()
    x_train = []
    y_train = []
    n_classes = 3
    for file_name in train_names:
        img = cv2.imread('C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//team-hasay-master//frames//' + file_name + '.png')
        if img.shape[0] != 256 or img.shape[1] != 256:
            img  = cv2.resize(img, (256, 256), interpolation = cv2.INTER_AREA)
        img = median_filter(img, size=3)
        img = normalize(img)
        x_train.append(img)
        img = cv2.imread('C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//team-hasay-master//masks//'  + file_name + '.png')
        if img.shape[0] != 256 or img.shape[1] != 256:
            img  = cv2.resize(img, (256, 256), interpolation = cv2.INTER_AREA)
        img = img[:,:,0]
        seg_mask = np.zeros((img.shape[0], img.shape[1], n_classes))
        for label in range(n_classes):
            seg_mask[:,:,label] = (img == label).astype(int)
        y_train.append(seg_mask)
    x_train = np.array(x_train)
    y_train = np.array(y_train)
    return x_train, y_train

def load_testing_data():
    test_names = open('C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//team-hasay-master//test.txt').read().split()
    x_test = []
    img_shapes = []
    n_classes = 3
    for file_name in test_names:
        img = cv2.imread('C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//team-hasay-FCN//frames_one_std//' + file_name + '.png')
        img_shapes.append(img.shape)
        if img.shape[0] != 256 or img.shape[1] != 256:
            img  = cv2.resize(img, (256, 256), interpolation = cv2.INTER_AREA)
        img = median_filter(img, size=3)
        img = normalize(img)
        x_test.append(img)
    x_test = np.array(x_test)
    return x_test, img_shapes, test_names
    
x_train, y_train = load_training_data()
x_test, img_shapes, test_names = load_testing_data()

model = unet(256, 256, 3)
model.summary()

sgd = optimizers.SGD(lr=0.3, decay=5**(-4), momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

model_path = 'C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//models//Best_Norm_One_STD.h5'
callbacks=[ModelCheckpoint(filepath=model_path, monitor='val_loss', save_best_only=True)]
model.fit(x_train, y_train, batch_size=2, epochs=10, validation_split=0.1, callbacks=callbacks)
model.save('C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//models//Full_Norm_One_STD.h5')

#Full
pred = model.predict(x_test)
pred_imgs = np.argmax(pred, axis=3)

#Work-around to resize and save images, due to cv2 bug
for i in range(len(pred_imgs)):
    output_path = 'C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//predictions//full_outputs//' + test_names[i] + '.png'
    cv2.imwrite(output_path, pred_imgs[i])
    if img_shapes[i][0] != 256 or img_shapes[i][1] != 256:
        img = cv2.imread(output_path)
        img = img[:,:,0]
        img = cv2.resize(img, (img_shapes[i][1], img_shapes[i][0]), interpolation=cv2.INTER_CUBIC)
        cv2.imwrite(output_path, img)
        
model_path = 'C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//models//Best_Norm_One_STD.h5'
#Best Val_loss
model = load_model(model_path)
pred = model.predict(x_test)
print(np.unique(pred_imgs))
pred_imgs = np.argmax(pred, axis=3)
print(np.unique(pred_imgs))

for i in range(len(pred_imgs)):
    output_path = 'C://Users//dhava//Desktop//My Courses//Data Science Practicum//Problem2//predictions//best_outputs//' + test_names[i] + '.png'
    cv2.imwrite(output_path, pred_imgs[i])
    if img_shapes[i][0] != 256 or img_shapes[i][1] != 256:
        img = cv2.imread(output_path)
        img = img[:,:,0]
        img = cv2.resize(img, (img_shapes[i][1], img_shapes[i][0]), interpolation=cv2.INTER_CUBIC)
        cv2.imwrite(output_path, img)
